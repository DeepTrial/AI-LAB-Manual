---
typora-copy-images-to: ..\img
---

# 集成学习

## 1. 集成学习概念

集成学习（ensemble learning）构建并组合多个学习器来完成学习任务。集成学习模型如图所示，先产生T个个体学习器，再依据某种策略将学习器集成，得到输出结果。

![image-20210119152348492](..\img\image-20210119152348492.png)

个体学习器可选择多种算法实现，如决策树（C4.5，ID3），神经网络。如果T个学习器采用相同的算法实现，则称为**同质集成**，反之则成为**异质集成**。

- 同质集成中的个体学习器可称为基学习器，对应算法为基学习算法
- 异质集成中的个体学习器可称为组件学习器



集成学习将多个学习器进行结合，可取得超过单一学习器的性能。多数研究采用弱学习器作为个体学习器，集成方法带来的效果提升明显。但在实际场景中，常使用较强的学习器。（西瓜书P172 给出 单一学习性能与集成模型性能 关系的例子）

> 弱学习器：泛化性能略优于随机猜测的学习器

理想情况下，集成学习的个体学习器应当“好而不同”，即个体学习器要有一定准确性（性能不能太差），也要有一定多样性（各个学习器要有差异不能完全相同）



## 2. Boosting与Bagging

根据个体学习器的生成方式，集成学习方法可以分为2大类：

- Boosting方法（提升方法）。个体学习器间存在依赖关系，必须串行生成各个学习器。典型方法：adaboost
- Bagging方法。个体学习器间不存在依赖关系，可以并行生成各个学习器。典型方法：random forest

### 2.1 Boosting算法机制

boosting方法（提升方法）的工作机制类似：

- 1. 从初始训练集训练一个基学习器
- 2. 根据当前基学习器的表现对训练样本分布进行调整，使判断错误的样本受到更多关注
- 3. 根据调整后的样本分布训练下一个学习器，重复步骤2-3直至得到T个学习器

- 4. 最终将T个学习器加权结合成集成模型

### 2.2 Bagging算法机制

bagging方法，又称装袋算法。bagging方法通过随机抽取T个训练子集，并行生成T个基学习器，每个基分类器又可以选择不同的算法实现（决策树，神经网络...）。根据生成子集的方式，bagging方法又可分为：

- Pasting：从所有样本中随机选取，生成T个子集
- Bagging：从所有样本中重置随机抽样（sample with replacement，有时用bootstrap表示），生成T个子集，不同子集可能包含相同样本
- Random Subspaces：从所有样本的特征中随机选取，生成T个子集
- Random Patches：从所有样本及其特征中随机选取，生成T个子集（综合Pasting和Random Subspaces）