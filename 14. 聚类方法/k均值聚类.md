# k均值聚类

k均值聚类是基于样本集合划分的聚类算法。该算法将样本集划分为k个子集，构成k个类。每个样本到其所属类的中心距离最小。每个样本只能属于一个类。

k均值聚类是基于划分的聚类方法，类别数k需要提前指定。由于采用了近似的迭代算法，所以不能保证得到全局最优解。

## 1. 模型定义

给定包含n个样本的集合$X=\{x_1,x_2,...,x_n\}$,每个样本由一个m维的特征向量表示。k均值聚类的目标是将n个样本分到k个不同的类别内。用C表示聚类算法的划分。一个划分对应一个聚类结果。

如果把每个样本用一个整数表示，每个类也用一个整数表示，则聚类就可以用函数表示：
$$
l=C(i)
$$
所以k均值聚类的模型是一个从样本到类的函数。



## 2. 策略

k均值聚类的策略是通过损失函数的最小化选取最优的划分或函数$C^*$。k均值聚类以欧氏距离作为样本间距离的度量标准，损失函数定义为每个样本与其各自所属的类的中心距离总和：
$$
W(C)=\sum_{l=1}^k\sum_{C(i)=l}||x_i-\bar{x}_l||^2
$$
其中内循环表示第$l$个类的距离总和。$W(C)$也称为能量，表示相同类的样本相似的程度。k均值聚类就是求解最优化问题：
$$
\begin{align}
C*&=\mathop{\arg \max}_{C}W(C)\\
&=\mathop{\arg \max}_{C}\sum_{l=1}^k\sum_{C(i)=l}||x_i-\bar{x}_l||^2
\end{align}
$$
当相似的样本被聚到同一类时，损失函数值最小。因此最优化损失函数可以实现聚类目标。然而，所有可能的划分方法为：
$$
S(n,k)=\frac{1}{k!}\sum_{l=1}^k(-1)^{k-l}A^k_lk^n
$$
所以最优化目标函数是NP-hard问题。现实中求解常使用迭代法近似。

## 3. 算法

算法的迭代过程包括两个步骤：

- 选择k个聚类中心，将样本指派到各个类内，得到一个聚类结果
- 更新类的样本均值作为新的聚类中心

重复上面两个步骤，直至收敛为止

具体过程如下：

- 1. 初始化。令$t=0$，随机选择k个样本点作为初始聚类中心$m^{(0)}=\{m_1^{0},m_2^{0},...,m_k^{0}\}$

- 2. 对样本进行聚类。对当前的类中心$m^{(t)}$,计算每个样本到类中心的距离，将每个样本指派给与其距离最近的类中心，构成聚类结果$C^{(t)}$

- 3. 计算新的类中心。根据聚类结果，计算当前各个类中的样本均值，作为新的类中心$m^{(t+1)}$
     $$
     m^{(t+1)}_l=\frac{1}{n_l}\sum_{C(i)=l}x_i, \quad l=1,2,...,k
     $$

- 4. 如果迭代收敛或符合停止条件，输出$C^*=C^{(t)}$。否则$t=t+1$,返回步骤2

此时k均值聚类的算法复杂度为$O(mnk)$



由于k值是提前人为指定的，而实际数据的最优k值是不知道的，因此需要用不同k值进行聚类，检验聚类的质量。通常使用类的平均直径作为衡量标准。

- 类别数变小时，平均直径会增加
- 类别数变大时，平均直径会降低，但到超过某个值后，平均直径会不变。而这个数就是最优k值。

可以使用二分查找快速找打最优的k值

