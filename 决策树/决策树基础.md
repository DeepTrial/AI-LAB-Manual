# 决策树

**决策树的定义**：

>  分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点包含2种类型：内部节点（表示一种特征或熟悉），叶结点（表示一个类）



决策树可以看作一个if-then规则的集合，也可以表示给定特征条件下类的条件概率分布。（具体参考统计学习方法P68的内容）

决策树是一种基本的分类和回归方法，其模型呈树状结构。决策树的主要优点是可读性高和分类速度快。决策树的学习过程通常包含3个步骤：特征选择、决策树生成、决策树的修剪。

决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。



## 1.特征选择

特征选择在于选取对训练数据具有分类能力的特征。利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。

常用的特征选择准则包括信息增益，信息增益比，基尼指数。首先需要明确的是熵和条件熵的概念：

熵$H(x)$：表示随机变量不确定性的度量。熵取值越大，随机变量的不确定性越大（越混乱）
$$
H(x)=-\sum_{i=1}^Np_ilogp_i
$$
条件熵$H(Y|X)$：表示在已知随机变量X的条件下随机变量Y的不确定度。计算方式同数学期望
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)\\
p_i=P(X=x_i)
$$


### 1.1 信息增益

定义：特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差。
$$
g(D,A)=H(D)-H(D|A)
$$
信息增益度量了由于特征A而使得对数据集D得分类得不确定性减少的程度。信息增益大的特征具有更强的分类能力。

对于数据集D，$|D|$表示样本容量。假设有K个类$C_k$，$|C_k|$表示属于该类别的样本个数。特征A有n个不同的取值，可以根据特征A的取值将D划分为n个不同的子集$D_1,D_2,...,D_n$，$|D_i|$表示对应子集的样本个数。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，$|D_{ik}|$表示对应集合的样本个数

具体计算过程如下：

- 计算数据集D的经验熵$H(D)$：
  $$
  H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
  $$

- 计算特征A对数据集D的经验条件熵$H(D|A)$：
  $$
  H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}
  $$

- 计算信息增益：
  $$
  g(D,A)=H(D)-H(D|A)
  $$




### 1.2 信息增益比

以信息增益作为划分特征存在偏向于取值较多的特征的问题。（例如标号，可以将数据逐条分开，且每条数据不可再分）信息增益比可以对该问题进行校正。

定义：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$

### 1.3 基尼指数

定义：分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
在特征A的条件下，集合D的基尼指数定义为：
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

## 2. 决策树生成

常用的决策树生成方法包括：ID3，C4.5，CART三种，具体方法将在后续单独介绍



## 3.决策树修剪

在决策树学习中将已生成的树进行简化的过程称为剪枝。决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数实现。



假设决策树T的叶结点个数为$|T|$，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个。$H_t(T)$为叶结点t上的经验熵，$\alpha$是非负参数，决策树的损失函数可以定义为：
$$
C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\
H_t(T)=-\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$
在损失函数中，可以将上式第一项定义为模型对训练数据的预测误差$C(T)$，表示模型与训练数据的拟合程度
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)
$$
式（10）的第二项表示模型的复杂度，参数$\alpha$控制两者的影响。较大的$\alpha$促使选择简单的模型，较小的$\alpha$促使选择较复杂的模型，取0时表示只考虑模型与训练数据的拟合程度。不考虑复杂度。



决策树的剪枝就是当参数$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。在生成过程中，只考虑了通过提高信息增益等数值对训练数据进行更好拟合，而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。

对于给定的决策树T，树的剪枝算法如下：

- 计算每个结点的经验熵

- 递归地从树的叶结点向上回缩。假设一组叶结点回缩到其父节点之前与之后的整体树为$T_B$和$T_A$，对应的损失函数值为$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果
  $$
  C_{\alpha}(T_A)<=C_{\alpha}(T_B)
  $$
  则进行剪枝，即将父结点变为新的叶结点

- 返回上一步骤，直至不能继续为止，得到损失函数最小的子树$T_{\alpha}$

