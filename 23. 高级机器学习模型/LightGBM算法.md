# LightGBM算法

基于梯度提升方法的集成决策树模型在机器学习中应用十分广泛。梯度提升方法在构建决策树时，需要遍历所有特征的所有取值，查找最优切分点。但是这种基于pre-sorted的方法十分耗时，在处理高维的大数据集时十分困难。LightGBM算法在查找特征的最优切分点时，没有使用基于pre-order的方法，而是使用了基于histogram的方法，大大加快运行速度，降低了内存消耗。



总的来说。LightGBM相当于XGBoost+Histogram+GOSS+EFB

若将lightgbm与XGBoost相比，lightgbm算法的改进之处包括：

- 采用基于直方图（histogram）的决策树算法。XGBoost使用基于pre-sorted方法查找最有特征的最优切分点，但面对高维大数据的处理速度较慢（相比基于直方图的方法）。
- 面对大量数据，设计了GOSS（基于梯度的one-side）采样方法提高训练速度。机器学习算法面对大数据量时候都会使用采样的方式（根据样本权值）来提高训练速度。
- 使用EFB（互斥特征捆绑）提高基于直方图的算法对稀疏数据的处理能力。基于直方图的方法在处理稀疏数据时效率较低，因此lightgbm设计了EFB方法处理稀疏数据。



## 1.基于直方图的方法

### 1.1直方图算法的思想

直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。

我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：

- **内存占用更小：**XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；在每次查找特征的最优切分点时，直方图算法不需要将所有数据读取进内存再排序，只需要存储直方图。

- **时间复杂度更小：**计算每个特征分裂增益时，XGBoost 需要遍历一次所有数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k个bin的直方图，因此时间复杂度从 $O(\#data*\#feature)$降低到 $O(k*\#feature)$。在LightGBM中，$\#data >> k$，所以时间复杂度降低十分明显。

  

直方图算法的流程如下：

![image-20210128161328567](../img/image-20210128161328567.png)



### 1.2 直方图算法的加速

在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。在实际操作过程中，我们还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。

![preview](../img/v2-66982f5386b2e9be3e50a651e01b9c21_r.jpg)

### 1.3 稀疏数据的处理

XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。

## 2.GOSS采样方法

GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。

GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。具体算法如下所示：

![image-20210128203102197](../img/image-20210128203102197.png)



我们可以看到 GOSS 事先基于梯度的绝对值对样本进行排序（**无需保存排序后结果**），然后拿到前 a% 的梯度大的样本，和总体样本的 b%，在计算增益时，通过乘上$\frac{1-a}{b}$ 来放大梯度小的样本的权重。**一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。**



## 3. EFB互斥特征捆绑

高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。

### 3.1 查找可以绑定的特征

EFB 算法利用特征和特征间的关系构造一个加权无向图，并将其转换为图着色算法。我们知道图着色是个 NP-Hard 问题，故采用贪婪算法得到近似解，具体步骤如下：

1. 构造一个加权无向图，顶点是特征，边是两个特征间互斥程度；
2. 根据节点的度进行降序排序，度越大，与其他特征的冲突越大；
3. 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。

算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大互斥率$\gamma$来平衡算法的精度和效率。EFB 算法的伪代码如下所示：

![image-20210128205617883](../img/image-20210128205617883.png)

我们看到时间复杂度为$O(\#feature^2)$ ，在特征不多的情况下可以应付，但如果特征维度达到百万级别，计算量则会非常大，为了改善效率，LightGBM提出了一个更快的解决方案：将 EFB 算法中通过构建图，根据节点度来排序的策略改成了根据非零值的技术排序，因为非零值越多，互斥的概率会越大。

### 3.2 特征值合并

论文给出特征合并算法，其关键在于原始特征能从合并后的捆绑特征中分离出来。假设 一个捆绑（Bundle） 中有两个特征值，A 取值范围为 [0, 10）、B 取值为 [0, 20），为了保证特征 A、B 的互斥性，我们可以给特征 B 添加一个偏移量10，将其取值范围转换为 [10, 30）。由于有偏移量的原因，特征A、B合并和不会产生冲突，且合并后的捆绑特征的取值范围为 [0, 30]，这样便实现了特征合并。具体算法如下所示：

![image-20210128210953724](../img/image-20210128210953724.png)

## 4. 参数注释

### 4.1 参数指南

完整的参数介绍可以参照lightgbm的手册

### 4.2 调参建议

**官方参数优化建议**

**针对 Leaf-wise (最佳优先) 树的参数优化**

- **num_leaves** . 这是控制树模型复杂度的主要参数. 理论上, 借鉴 depth-wise 树, 我们可以设置 num_leaves = 2^(max_depth) 但是, 这种简单的转化在实际应用中表现不佳. 这是因为, 当叶子数目相同时, leaf-wise 树要比 depth-wise 树深得多, 这就有可能导致过拟合. 因此, 当我们试着调整 num_leaves 的取值时, 应该让其小于 2^(max_depth). 举个例子, 当 max_depth=6 时(这里译者认为例子中, 树的最大深度应为7), depth-wise 树可以达到较高的准确率.但是如果设置 num_leaves 为 127 时, 有可能会导致过拟合, 而将其设置为 70 或 80 时可能会得到比 depth-wise 树更高的准确率. 其实, depth 的概念在 leaf-wise 树中并没有多大作用, 因为并不存在一个从 leaves 到 depth 的合理映射.
- **min_child_samples** （ **min_data_in_leaf** ）. 这是处理 leaf-wise 树的过拟合问题中一个非常重要的参数. 它的值取决于训练数据的样本个树和 num_leaves. 将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合. 实际应用中, 对于大数据集, 设置其为几百或几千就足够了.
- **max_depth** . 你也可以利用 max_depth 来显式地限制树的深度

**针对更快的训练速度**

- 通过设置 **subsample** （ **bagging_fraction** ） 和 **subsample_freq（=** **bagging_freq）** 参数来使用 bagging 方法进行采样提升训练速度(减小了数据集)
- 通过设置 **colsample_bytree（= feature_fraction）** 参数来使用特征的子抽样
- 使用较小的 **max_bin，** 较少的直方图数目
- 使用 **save_binary** 将数据集被保存为二进制文件，下次加载数据时速度会变快
- 通过并行训练来提速

**针对更好的准确率**

- 使用较大的直方图数目 **max_bin** ，这样会牺牲训练速度
- 使用较小的学习率 **learning_rate** ，这样会增加迭代次数
- 使用较大的 **num_leaves，** 可能导致过拟合
- 使用更大的训练数据
- 尝试 **dart** 模型（Dropouts meet Multiple Additive Regression Trees）

**处理过拟合**

- 设置较少的直方图数目 **max_bin**
- 设置较小的叶节点数 **num_leaves**
- 使用 **min_child_samples（min_data_in_leaf）** 和 **min_child_weight（= min_sum_hessian_in_leaf）**
- 通过设置 **subsample（bagging_fraction）** 和 **subsample_freq（=** **bagging_freq）** 来使用 bagging
- 通过设置 **colsample_bytree（feature_fraction）** 来使用特征子抽样
- 使用更大的训练数据
- 使用 **reg_alpha（lambda_l1）** , **reg_lambda（lambda_l2）** 和 **min_split_gain（min_gain_to_split）** 来使用正则
- 尝试 **max_depth** 来避免生成过深的树

##  参考文献

手册：https://lightgbm.readthedocs.io/en/latest/

中文文档：https://lightgbm.apachecn.org/

1. https://blog.csdn.net/qq_24519677/article/details/82811215
2. https://blog.csdn.net/maqunfi/article/details/82219999
3. https://zhuanlan.zhihu.com/p/87885678
4. https://blog.csdn.net/anshuai_aw1/article/details/83040541

调参指南：

1. https://www.pythonf.cn/read/6998
2. https://blog.csdn.net/u012735708/article/details/83749703

