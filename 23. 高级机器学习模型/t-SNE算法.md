# t-SNE算法

t-SNE(t-distributed stochastic neighbor embedding)是用于**降维**的一种机器学习算法，是由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。此外，t-SNE 是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。相比于常用的PCA降维，两者的区别是：

- PCA降维：线性降维方法。将高维数据降至低维（例如从100维降至20-30维），便于后续处理
- t-SNE降维：非线性降维方法。通常将数据降低至2-3维，只要是为了数据可视化

t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。

## 1. SNE算法

### 仿射变换

![image](../img/120296-20160218190315722-2110673980.png)

仿射变换(Affine Transformation) 是一种二维坐标到二维坐标之间的线性变换，保持二维图形的“平直性”（译注：straightness，即变换后直线还是直线不会打弯，圆弧还是圆弧）和“平行性”（译注：parallelness，其实是指保二维图形间的相对位置关系不变，平行线还是平行线，相交直线的交角不变）  。

### 1.1 基本思想

SNE是通过仿射变换将数据点映射到概率分布上，主要包括两个步骤：

- SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。
- SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。

t-SNE模型是非监督的降维聚类，跟kmeans等不同是，算法不能通过训练确定参数之后再用于其它数据（比如kmeans可以通过训练得到k个点，再用于其它数据集，而t-SNE只能单独的对数据做操作，也就是说他只有fit_transform，而没有fit操作）

### 1.2 问题建模

SNE是先**将欧几里得距离转换为条件概率来表达点与点之间的相似度**。具体来说，给定一个N个高维的数据$x_1,...,x_N$（注意N不是维度）, t-SNE首先是计算概率$p_{ij}$,正比于$x_i$和$x_j$之间的相似度（这种概率是我们自主构建的），即：
$$
p_{j∣i}=\frac{exp(−||x_i−x_j||^2/(2σ^2_i))}{∑_{k≠i}exp(−||x_i−x_k||^2/(2σ^2_i))}
$$
这里的有一个参数是$σ_i$，对于不同的点$x_i$取值不一样，后续会讨论如何设置。此外设置$p_{x∣x}=0$,因为我们关注的是两两数据对之间的相似度。

那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\frac{1}{\sqrt{2}}$，因此它们之间的相似度如下:
$$
q_{j∣i}=\frac{exp(−||x_i−x_j||^2)}{∑_{k≠i}exp(−||x_i−x_k||^2)}
$$
同样，设定$q_{i∣i}=0$.

如果降维的效果比较好，局部特征保留完整，那么 $p_{i∣j}=q_{i∣j}$, 因此算法优化两个分布之间的距离-KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:
$$
C=∑_iKL(P_i∣∣Q_i)=∑_i∑_jp_{j∣i}log\frac{p_{j∣i}}{q_{j∣i}}
$$
这里的$P_i$表示了给定点$x_i$下，其他所有数据点的条件概率分布。需要注意的是**KL散度具有不对称性**，在低维映射中不同的距离对应的惩罚权重是不同的，具体来说：

- 原始距离较远的两个点降维后距离较近则产生的cost相对较大，
- 原始距离较近的两个点降维后距离较远则产生的cost相对较小(注意：类似于回归容易受异常值影响，但效果相反)。

因此，**SNE会倾向于保留数据中的局部特征**。

### 1.3 求解过程

下面我们开始正式的推导SNE。首先不同的点具有不同的$σ_i$，$P_i$的熵(entropy)会随着$σ_i$的增加而增加。SNE使用困惑度([perplexity](https://en.wikipedia.org/wiki/Perplexity))的概念，用二分搜索的方式来寻找一个最佳的$σ_i$。其中困惑度指:
$$
Perp(P_i)=2^{H(P_i)}
$$
这里的$H(P_i)$是$P_i$的熵，即:
$$
H(P_i)=−∑_jp_{j∣i}log_2p_{j∣i}
$$
困惑度可以解释为一个点附近的有效近邻点个数。**SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间**，给定困惑度的值之后，使用二分搜索的方式寻找合适的$σ_i$

求解的核心问题是如何求解梯度。目标函数等价于$∑∑−plog(q)$这个式子与softmax非常的类似，我们知道softmax的目标函数是$∑−ylogp$，对应的梯度是$y−p$(注：这里的softmax中$y$表示$label$，$p$表示预测值)。 同样我们可以推导SNE的目标函数中$i$在$j$下的条件概率情况的梯度是2(pi∣j−qi∣j)(yi−yj)2(pi∣j−qi∣j)(yi−yj)， 同样j在i下的条件概率的梯度是2(pj∣i−qj∣i)(yi−yj)2(pj∣i−qj∣i)(yi−yj), 最后得到完整的梯度公式如下:

δCδyi=2∑j(pj∣i−qj∣i+pi∣j−qi∣j)(yi−yj)δCδyi=2∑j(pj∣i−qj∣i+pi∣j−qi∣j)(yi−yj)

在初始化中，可以用较小的σσ下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:

Y(t)=Y(t−1)+ηδCδY+α(t)(Y(t−1)−Y(t−2))Y(t)=Y(t−1)+ηδCδY+α(t)(Y(t−1)−Y(t−2))

这里的Y(t)Y(t)表示迭代t次的解，ηη表示学习速率,α(t)α(t)表示迭代t次的动量。

此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。

## 2. t-SNE算法

## 参考文献

1. https://www.zhihu.com/question/52022955/answer/387753267
2. http://www.datakit.cn/blog/2017/02/05/t_sne_full.html#11%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86