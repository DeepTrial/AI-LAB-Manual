# Normalization

## Batch Normalization

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

### Internal Covariate Shift问题

ICS问题指的是神经网络的中间层的输入的分布变化。假设一个最简单的4层神经网络如下图所示：



![img](https://pic2.zhimg.com/v2-1d48853107f5e848214e9078bbf8441d_b.jpg)



再假设我们要优化的损失函数为L，则L相对于d层神经元的参数$w_d$的偏导数的计算公式如下(链式法则)，其中$z_d = w_d * z_c$(z是网络层的输出，简单起见省略了激活函数)。根据公式可以看出：d层神经元的权重的梯度取决于上一层c层神经元的输出。

![img](https://pic3.zhimg.com/v2-4e69bcb66d27748b2078f76a531f3d4e_b.jpg)

将上面得到的结论推广开来，有：

>  **某一层神经元的权重的梯度取决于该层神经元的输入，也即取决于上层神经元的输出。**

在反向传播时，聚焦到d层神经元上，我们使用梯度对参数作了更新后，希望新的参数会带来更小的loss，但实际可能与我们的期望不符，因为：

- 第$i$轮迭代时，假设c层神经元的输出的分布为$p_i$，d层神经元的参数梯度由$p_i$求得，并对参数$w_d$作更新。

- 第i轮的梯度反向传播，对c层神经元的参数w_c也作了更新，将会导致c层的输出分布变化。

- 第$i+1$轮时，c层神经元的输出分布由原来的$p_i$变成了$p_{i+1}$。由于第i轮时，d层神经元的参数是依据$p_i$更新的，分布从$p_i$到$p_{i+1}$的变化，**可能导致第i+1轮计算的loss可能不会减小**。（另外从I.I.D.角度分析，）

### BN如何解决ICS(Internal Covariate Shift)？

一种最基本的解决办法就是对网络的**输入**作归一化(Normalization)，使得输入分布的均值为0，标准差为1（类似于白化操作）。然而这个方法仅在网络不深的情况下才奏效；一旦网络是比较深的，假设有20层，**其中每一层参数引起的分布的微小变化迭加起来是巨大的**。可以拿方言随距离演变的例子帮助理解：相邻城市之间的方言的差异基本很小，例如北京话和天津话；距离逐渐增大会导致方言逐渐变化，当距离很大时，方言之间的差异也会变得很大，例如北京话和广州话。

接下来自然就引入了BN(Batch Normalization)的概念，即对每一层的输入都作归一化(Normalization)，但除此之外还有一些别的操作，公式可以给出最精准的描述：

![img](F:\Projects\AI-LAB-Manual\img\v2-968c341b0ab398b209de64da5c541d48_b.jpg)

公式描述了如何在一个batch内计算每一层的均值，标准差，之后减均值，除以标准差，将分布转换为均值为0，标准差为1的标准分布。公式5是BN中最重要的部分，其中γ和β是BN层中的参数，经过公式5后，分布的均值为变为β，标准差为γ（β，γ是BN层可学习的参数）。整体上看，**BN层的作用就是通过参数控制了每一层输出的均值和标准差**。且在特定条件下，BN层有能力恢复输出的原始分布。

### BN的本质

BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），

![img](F:\Projects\AI-LAB-Manual\img\v2-506079547799276bbf97dfd9bfdb1307_b.jpg)

所以这导致后向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正太分布而不是正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

## LN、IN、GN

![preview](F:\Projects\AI-LAB-Manual\img\v2-66b2a13334967dc27025e354bb448875_r.jpg)

带有BN层的网络错误率会随着batch_size的减小而迅速增大，当我们硬件条件受限不得不使用较小的batch_size时，Layer Norm，Instance Norm、Group Norm就可以派上用处。

- BN：沿着channel通道，对一个batch内的特征计算均值和标准差
- LN：沿着batch，对一条数据的特征进行计算均值和标准差。**适合于序列数据和生成模型**
- IN：对一条数据的一个通道计算均值和标准差。**适合于序列数据和生成模型**
- GN：对channel通道分组，对一条数据的各组channel计算均值和标准差

