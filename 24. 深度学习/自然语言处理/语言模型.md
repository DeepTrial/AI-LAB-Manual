# 语言模型

语言模型用于计算一个给定句子合理（文字组合是否常见）的概率，对于一个语句$w_1,w_2,...,w_k$，语言模型计算的概率$P(w_1,w_2,...,w_k)$在区间$[0,1]$之间，值越大表示句子越合理。

语言模型可分为统计语言模型和神经网络语言模型两种。

## 统计语言模型

对于语句$S=w_1,w_2,...,w_k$，一种直观的概率计算方法是
$$
P(S)=P(w_1,w_2,...w_k)=P(w_1)P(w_2|w_1)...P(w_k|w_1,w_2,..,w_k-1)
$$
这中计算方式包含明显的问题：

- 参数空间大：最后几项的条件可能性很多，无法计算
- 数据稀疏：很多长词对组合在语料库中很少见，由此得到的概率会趋向于0，失真

### 马尔可夫假设与n-gram模型

为了解决上述问题，人们引入了马尔可夫链假设，即当前词出现的概率只和其前面有限的n-1项相关。基于此假设的模型为n-gram语言模型（n元语言模型），一般n取1（uni-gram 1元语言模型）或2（bi-gram 2元语言模型），当n大于4时，n-gram模型的优势就不明显了。

与2元语言模型为例，其计算过程如下：
$$
P(S)=P(w_1,w_2,...,w_k)\approx P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_k|w_{k-1})
$$
对于n-gram语言模型中的一项$P(w_{i+1}|w_{i-k},...,w_i)$
$$
P(w_{i+1}|w_{i-k},...,w_i)=\frac{\#(w_{i-k},...w_{i+1})}{\sum_{w_j}\#(w_{i-k},...,w_i,w_j)}=\frac{\#(w_{i-k},...w_{i+1})}{\#(w_{i-k},...,w_i)}
$$
其中$\#()$表示该词对出现的次数，第一个等式后的式子表示条件概率的计算过程，第二个等式为推论公式。

当n-gram模型的n取大于1的值时，通常会给原始词词序列加上一个或多个起始符$<s>$，因此句首的词也能计算$P(w_1|<S>,...,<S>)$，让形式统一。

此外还会在句尾添加一个结束符$</s>$，**式3成立的原因也是因为存在结束符，如果没有结束符存在，n-gram模型只能计算固定长度的句子，而不是任意长度的序列(即 式3的第二个等式不成立)**



### 平滑技术与Back-off

观察式3的计算过程会发现，当分母上的词对没有出现时（未登录词<OOV>），语言模型的中这一项是无法计算的，这也是自然语言处理的一大痛点。为了解决此问题，有2种处理方法：

- 平滑技术：在分母上加上常数使其不可能为0
- back-off：如果分母上的词对没有出现过，则可以估计n-gram这一项的值，例如用n-1-gram估计该项n-gram的值，back-off的方法很多，可以自行了解

### 困惑度评价方法

语言模型的评价可采用困惑度perplexity计算：
$$
Perplexity=2^{-l}\\
l=\frac{1}{m}\sum_{i=1}^mp(x)log\;p(x)
$$
此时困惑度可以理解为语言模型在计算词对的混乱程度，取值范围在$[1,+\infin]$。当一个词对越混乱，其困惑度越高，语言模型的效果越差。

另一种困惑度的定义方法是：
$$
Perplexity(S)=P(w_1,w_2,...,w_k)^{-\frac{1}{n}}=\sqrt[n]{\prod_{i=1}^n\frac{1}{p(w_{i+1}|w_{i-k},...,w_i-1)}}
$$
此时困惑度可以理解为平均分支系数（average branching factor），即模型预测下一个单词时的平均可选择的单词数量，取值范围在$(0,+\infin]$。与之前相同，当一个词序越不合理，其困惑度就越大。

在比较不同的语言模型时，通常使用模拟真实环境的正常句子作为测试样例，计算样例在不同语言模型下的困惑度，困惑度越小表示语言模型效果越好。

### 统计方法的缺陷

基于统计方法的语言模型理论简单易懂，但也面临以下挑战：

- 计算复杂度大。对于一个词表规模为V的语料库而言，n-gram模型计算数目为$|V|^n$,当n变化时，计算量是指数级变化的。
- 人工设计的规则变化很多，但效果不一。平滑技术、back-off等方法有多种选择，但哪一种是最合适还未有定论。此外，困惑度评价体系本身也不是一个完美的评价方法
- 统计方法比较死板。原始语料库中未出现的词对会被判断为不合理词对，影响模型的估计，因此模型显得比较保守。但从另一方面来看，这种保守虽然降低了召回率，但却能保持较高的精度。



## 神经网络语言模型

观察n-gram语言模型中的一项：
$$
P(w_{i+1}|w_{i-k},...,w_i)
$$
可以理解为在给定一个n-1长度的序列后预测下一词为$w_{i+1}$的概率，这也是神经网络语言模型的本质。

