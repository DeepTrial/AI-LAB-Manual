# 语言模型

语言模型用于计算一个给定句子合理（文字组合是否常见）的概率，对于一个语句$w_1,w_2,...,w_k$，语言模型计算的概率$P(w_1,w_2,...,w_k)$在区间$[0,1]$之间，值越大表示句子越合理。

语言模型可分为统计语言模型和神经网络语言模型两种。

## 统计语言模型

对于语句$S=w_1,w_2,...,w_k$，一种直观的概率计算方法是
$$
P(S)=P(w_1,w_2,...w_k)=P(w_1)P(w_2|w_1)...P(w_k|w_1,w_2,..,w_k-1)
$$
这中计算方式包含明显的问题：

- 参数空间大：最后几项的条件可能性很多，无法计算
- 数据稀疏：很多长词对组合在语料库中很少见，由此得到的概率会趋向于0，失真

### 马尔可夫假设与n-gram模型

为了解决上述问题，人们引入了马尔可夫链假设，即当前词出现的概率只和其前面有限的n-1项相关。基于此假设的模型为n-gram语言模型（n元语言模型），一般n取1（uni-gram 1元语言模型）或2（bi-gram 2元语言模型），当n大于4时，n-gram模型的优势就不明显了。

与2元语言模型为例，其计算过程如下：
$$
P(S)=P(w_1,w_2,...,w_k)\approx P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_k|w_{k-1})
$$
对于n-gram语言模型中的一项$P(w_{i+1}|w_{i-k},...,w_i)$
$$
P(w_{i+1}|w_{i-k},...,w_i)=\frac{\#(w_{i-k},...w_{i+1})}{\sum_{w_j}\#(w_{i-k},...,w_i,w_j)}=\frac{\#(w_{i-k},...w_{i+1})}{\#(w_{i-k},...,w_i)}
$$
其中$\#()$表示该词对出现的次数，第一个等式后的式子表示条件概率的计算过程，第二个等式为推论公式。

当n-gram模型的n取大于1的值时，通常会给原始词词序列加上一个或多个起始符$<s>$，因此句首的词也能计算$P(w_1|<S>,...,<S>)$，让形式统一。

此外还会在句尾添加一个结束符$</s>$，**式3成立的原因也是因为存在结束符，如果没有结束符存在，n-gram模型只能计算固定长度的句子，而不是任意长度的序列(即 式3的第二个等式不成立)**



### 平滑技术与Back-off

观察式3的计算过程会发现，当分母上的词对没有出现时（未登录词<OOV>），语言模型的中这一项是无法计算的，这也是自然语言处理的一大痛点。为了解决此问题，有2种处理方法：

- 平滑技术：在分母上加上常数使其不可能为0
- back-off：如果分母上的词对没有出现过，则可以估计n-gram这一项的值，例如用n-1-gram估计该项n-gram的值，back-off的方法很多，可以自行了解

### 困惑度评价方法

语言模型的评价可采用困惑度perplexity计算：
$$
Perplexity=2^{-l}\\
l=\frac{1}{m}\sum_{i=1}^mp(x)log\;p(x)
$$
此时困惑度可以理解为语言模型在计算词对的混乱程度，取值范围在$[1,+\infin]$。当一个词对越混乱，其困惑度越高，语言模型的效果越差。

另一种困惑度的定义方法是：
$$
Perplexity(S)=P(w_1,w_2,...,w_k)^{-\frac{1}{n}}=\sqrt[n]{\prod_{i=1}^n\frac{1}{p(w_{i+1}|w_{i-k},...,w_i-1)}}
$$
此时困惑度可以理解为平均分支系数（average branching factor），即模型预测下一个单词时的平均可选择的单词数量，取值范围在$(0,+\infin]$。与之前相同，当一个词序越不合理，其困惑度就越大。

在比较不同的语言模型时，通常使用模拟真实环境的正常句子作为测试样例，计算样例在不同语言模型下的困惑度，困惑度越小表示语言模型效果越好。

### 统计方法的缺陷

基于统计方法的语言模型理论简单易懂，但也面临以下挑战：

- 计算复杂度大。对于一个词表规模为V的语料库而言，n-gram模型计算数目为$|V|^n$,当n变化时，计算量是指数级变化的。
- 人工设计的规则变化很多，但效果不一。平滑技术、back-off等方法有多种选择，但哪一种是最合适还未有定论。此外，困惑度评价体系本身也不是一个完美的评价方法
- 统计方法比较死板。原始语料库中未出现的词对会被判断为不合理词对，影响模型的估计，因此模型显得比较保守。但从另一方面来看，这种保守虽然降低了召回率，但却能保持较高的精度。



## 神经网络语言模型

观察n-gram语言模型中的一项：
$$
P(w_{i+1}|w_{i-k},...,w_i)
$$
可以理解为在给定一个n-1长度的序列后预测下一词为$w_{i+1}$的概率，这也是神经网络语言模型的本质。

### NNLM

2003年Bengio的文章**A Neural Probabilistic Language Model**是第一篇使用神经网络搭建语言模型的文章。NNLM模型的主要任务是利用给定的前n-1个词汇预测第n个词汇。此外这篇文章也第一次提出了**词向量概念：使用低维、稠密、连续的向量表达文本**

![img](../../img/v2-668ee005a47991cfbb26e6fa73425895_b.jpg)

**输入层**：以n-1个单词的one-hot编码为输入，每个one-hot编码为$|V|$维，V为词表大小（词汇总数量），one-hot编码的形式为：
$$
[0,....,0,1,0,...,0]
$$
这稀疏的高维编码会与维度为$|V|\times m$的embedding矩阵C相乘，每个one-hot编码在相乘后变为维度为m（m远小于|V|）的分布向量（distribution vector），实现降维目的。因此矩阵C可看作是一个查找表，存储了对应词汇的**词向量**，当有词汇输入时，更具one-hot编码的形式提取出对应的行向量作为词向量。矩阵C在训练阶段由反向传播所发不断优化，提升表达能力。

![preview](../../img/v2-1f8c3087c1c158cb5edd8f5dcb17bf3e_r.jpg)



**隐含层**：在得到n-1个词向量后，为了便于后续的计算，先将他们concat在一起，得到$n(m-1)\times 1$的向量x

隐含层的计算为简单的全连接，激活函数为tanh，计算过程为：
$$
hidden=tanh(Hx+d)
$$
矩阵H的维度为$h\times (n-1)m$，d维度为$h\times 1$

**输出层**：利用了隐含层的输出以及原始合并词向量，最后套上softmax, 其公式为：
$$
y=b+Wx+Utanh(Hx+d)\\
p(w_t|w_{t-1},...,w_{t-n+1})=\frac{e^{y_{w_t}}}{\sum_{i=1}^{n-1}e^{y_{w_i}}}
$$
其中矩阵U维度为$|V|\times h$，矩阵W维度为$|V|\times (n-1)m$，b维度为$|V|\times 1$,输出层预测了此表内每个词作为前n-1个词的后文的概率。

损失函数：
$$
L=\frac{1}{T}\sum_t logP(w_t|w_{t-1},...,w_{t-1+n})+R(\theta)
$$


