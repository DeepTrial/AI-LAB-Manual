# 词向量

词向量的表示是NLP中的基本任务，最在在NNLM中作为模型的副产物出现，现在对于词向量的研究已经十分全面。

## One-hot Code

表达词向量的最简单的方法就是One-hot编码，形式如下：
$$
[0,...,0,1,0,...,0]
$$
编码的维度与语料库中的总词数相同，且仅有1个维度为1，可看作是表示某个词出现与否，出现则将对应位置置为1。例如：

```
i:      [1, 0, 0, 0, 0]
you:    [0, 1, 0, 0, 0]
like:   [0, 0, 1, 0, 0]
apple:  [0, 0, 0, 1, 0]
banana: [0, 0, 0, 0, 1]
```



这样的编码生成简单，但实际意义不大。one-hot编码存在的缺陷包括：

- 高维度、稀疏：one-hot编码的维度过大，可能会有上万，需要庞大的计算资源。向量中只有1维是非零值，是十分稀疏的
- 缺少语义信息。one-hot编码表示的词无法判断词与词之间的关系，如相似、相反，因为每个向量都是正交的，欧氏距离也相同。

与此one-hot编码只适合做为预处理的输入，不会真正用于表示词向量

## TF-IDF

TF（Term Frequency，缩写为TF）是词频，即一个词在文中出现的次数，统计出来就是词频TF，显而易见，一个词在文章中出现很多次，那么这个词肯定有着很大的作用，但在统计时需要先将一些无意义的助词（停用词）去除，例如the a and等。TF的计算方式可选择一下任意一种：
$$
TF=某个词出现的次数\\
TF=\frac{某个词出现的次数}{文章的总词数}\\
TF=\frac{某个词出现的次数}{文章中出现最多的词的次数}\\
$$
IDF （Inverse Document Frequency，缩写为IDF）逆文档频率，一个重要性调整系数，衡量一个词是不是常见词。**如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。** 因此我们给这样的词一个比较大的权重，即逆文档频率，计算方式如下：
$$
IDF=log(\frac{文档总数}{包含该词的文档数+1})
$$
最后TF-IDF就是TF与IDF的乘积，该值与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。



## 共现矩阵

为了弥补one-hot编码语义信息的缺失，可以采用共现矩阵的方法。

通过考虑词和词的共现问题，可以反应词之间的语义关系。最简单的方法是使用基于文档的方式来表示词向量，其基本思想是如果两个词经常共同出现在多篇文档中，则说明这两个词在语义上紧密关联。基于文档的词向量可以反应出相关词的语义关系，但是随着文档规模的增大，向量的维度也相应增加，存在维度变化问题。

可以通过统计一个事先指定大小的窗口内单词的共现次数，来解决维度变化问题。这种方法以单词周边的共现词的次数做为当前的词向量。 比如有以下四句话的一个语料库：

```
我\喜欢\摄影
我\爱\运动
我\爱\学习
我\喜欢\画画
```

假设考虑的窗口大小为1，也就是说一个词只与它前面及后面的词相关，比如"我 爱"共现次数为2，则共现矩阵如下所示：

![img](../../img/v2-2b4429a498b1a6e06f323401fdc793b8_b.jpg)

这样，共现矩阵的行(或列)可表示为对应的词向量。如"我"的词向量为[0,2,2,0,0,0,0]。 同时可以知道"爱"，"喜欢"的词向量相似度较高，他们具有相近的意思。

上述矩阵是一个$n\times n$的对称矩阵$X$，矩阵维数随着词典数量n的增大而增大，可以使用**奇异值分解SVD、PCA**将矩阵维度降低。但是仍存在问题：

- 矩阵$X$的维度经常改变
- 由于大部分词并不共现而导致的稀疏性
- 矩阵维度过高带来的高计算复杂度

## 语言模型与神经网络

为了进一步解决词向量的生成问题，可以从语言模型角度入手，在构建语言模型的过程中的到词向量。

### NNLM

nnlm开创了分布式词向量表示的先后，后续许多模型以此为出发点，不断改进模型结构，得到更好的效果。

![img](../../img/v2-668ee005a47991cfbb26e6fa73425895_b.jpg)

NNLM在语言模型中已有介绍，这里不再重复。

### Word2Vec

Word2Vec包含2种训练模型，分别是CBOW和Skip-Gram，两者区别在于：

- CBOW模型根据中心词$W(t)$周围（左右两边）的词来预测中心词
- Skip-Gram模型根据中心词$w(t)$预测周围词

两者模型结构仅是输入层和输出层不同。参考文献：https://cs224d.stanford.edu/lecture_notes/notes1.pdf

#### CBOW模型

![img](../../img/v2-2a319bac1bb7fcae2f4395d2c38674ea_720w.jpg)

模型流程

- 输入层：上下文单词的onehot.  {假设单词向量空间dim为V，上下文单词个数为C}

- 所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，一般远小于V，初始化权重矩阵W}

- 所得的C个向量 {因为是onehot所以相乘得到的结果维度为$1\times N$} 相加求平均作为隐层向量, size为1*N.

- 乘以输出权重矩阵W' {N*V}

- 得到向量 {1*V} 激活函数处理得到V-dim概率分布  {PS: 因为是onehot嘛，其中的每一维度代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）

- 与true label的onehot做比较，误差越小越好

$V\times N$的矩阵W就是look-up table，将任何一个单词的one-hot编码与w相乘都将得到编码后的低维词向量

#### Skip-Gram模型

![img](../../img/v2-e164fe32c96307f30ca507b17ef7eeda_b.png)

Skip-Gram在生成训练样本时，以中心词为中心，划定一个窗口，并以窗口内的词组成word-pair作为训练样本

![img](/Users/resnick/Documents/Matrix/AI-LAB-Manual/img/v2-eff880fe468a849ee0a5114f0e2b9c4c_b.png)

skip-gram模型的特点：

- 正样本采样：是一种数据集清洗方式。因为语料中会出现大量的冠词，<question, a>, <question, the>等，这些样本基本上无法为Skip-Gram的重大假设：上下文词语当前词表示接近。因此应当对样本进行重采样，把高频词对频度降低，并且保留低频词对。具体的采样概率服从：
  $$
  P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}
  $$
  其中$f(w_i)$代表词$w_i$的出现频率，t是可以调整的超参数。

- 对于输入的中心词one-hot编码，维度为$1\times |V|$，首先与输入层的嵌入矩阵$W$（维度为$|V|\times d$，词向量查找矩阵）相乘，得到编码后的中心词词向量，维度为$1\times d$，然后再将上下文单次乘以输出层矩阵$W^{'}$，维度为$|V|\times d$（与嵌入矩阵维度相同，但并不是同一个矩阵），得到编码后的上下文词向量，计算2个词向量的logsigmoid损失

- 负采样：用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。skip-gram使用一元模型分布（unigram distribution）”来选择“negative words”，具体概率为：
  $$
  P(w_i)=\frac{f(w_i)^{3/4}}{\sum_{j=0}^n(f(w_j)^{3/4})}
  $$
  

![img](../../img/v2-2067caf4a7d5a20f7a296ea0cc74a1e5_b.png)



### GloVe

参考文献：http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf

- Word2Vec是一种基于局部上下文的预测模型，优点是可以概括比相关性更为复杂的信息，进行word analogy等任务时效果较好，缺点是对统计信息利用的不够充分；

- 共现矩阵以及使用SVD处理共现矩阵的方法属于全局统计模型，优点是训练快速，并且有效的利用了统计信息，缺点是对于高频词汇较为偏向，并且仅能概括词组的相关性。有的时候产生的word vector对于解释词的含义如word analogy等任务效果不好

GloVe（Global Vectors）将两者的优势结合，可以有效的利用全局的统计信息。

首先构造共现概率矩阵（co-occurrence probability matrix），对于矩阵$X$，$X_{ij}$表示单词$j$出现在单词$i$上下文种的次数，$X_i=\sum_kX_{ik}$代表所有出现在单词$i$的上下文中的单词次数，$P_{ij}=P(j|i)=X_{ij}/X_i$代表单词$j$出现在单词$i$上下文中的概率，如图所示：

![img](../../img/v2-22d0f7970cbe4ec238ef55ff746125da_b.jpg)

![image-20210420102514502](../../img/image-20210420102514502.png)

![preview](../../img/v2-33e51493fa8e2170bba8df468e41e685_r.jpg)

![preview](../../img/v2-11fe585fa53b9e54d94c9c996187bf89_r.jpg)

![preview](../../img/v2-fef86f8c0487368408f20e453727104b_r.jpg)

在模型实现时，需要先计算共现矩阵，然后搭建GloVe模型，$w_i,w_k，w_j$这些就是我们需要的词向量。

### ELMo



### GPT

### Bert



