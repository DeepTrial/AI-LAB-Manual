# 提升树算法

提升树（boosting tree）常被认为是统计学习中性能最好的方法之一。提升树以分类树或回归树为基本分类器的提升（boosting）方法。

提升（boosting）方法实际采用加法模型与前向分布算法实现。以决策树为基函数的boosting方法被称为提升树。对于分类问题，决策树是二分类树。对于回归问题，决策树是二叉回归树。提升树模型可表示为：
$$
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
$$
其中$T(x;\Theta_m)$是决策树，$\Theta_m$是决策树的参数，$M$是树的个数

提升树的算法流程可概括为：

- （a）初始提升树$f_0(x)=0$

- （b）循环迭代多次，构造多颗决策树。对于第m次，提升树模型为：
  $$
  f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
  $$
  其中$f_{m-1}(x)$为当前提升树模型，为了确定当前决策树的参数$\Theta_m$，需要最小化经验风险。
  $$
  \tilde{\Theta_m}=\mathop{\arg\min}_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
  $$

- （c）将最终的提升树模型最为结果输出



**不同问题的提升树算法区别在于使用的损失函数不同。**回归问题常用平方损失函数，分类问题常用指数损失函数。同时还可以使用一般的损失函数处理不的问题。

## 二分类问题

对于二分类问题的提升树算法只需要将AdaBoost中的基分类器限制为二分类器即可。因此，提升树算法可认为是adaboost算法的特殊情况。



## 回归问题

回归树模型的思想是将输入空间划分为互不相交的区域$R$，并在每个区域上确定一个输出常量$c_j$, 回归树模型可表示为：
$$
T(x;\Theta)=\sum_{j=1}^Jc_jI(x \in R_j)
$$
 其中参数$\Theta=\{(R_1,c_1),(R_2,c_2),...,(R_J,c_J)\}$表述树的区域划分和各区域上的常数，$J

$是叶节点个数。

在迭代过程中（上文步骤（b）），为了求解参数$\Theta$时，最小化经验风险即平方损失函数:
$$
L(y,f_{m-1}(x)+T(x;\Theta_m))=[y-f_{m-1}(x)-T(x;\Theta_m)]^2=[r-T(x;\Theta_m)]^2
$$
其中$r=y-f_{m-1}(x)$是当前拟合模型的残差。由此可见，回归问题的提升树算法只需要简单拟合当前模型的残差即可。具体步骤如下：

- 初始化$f_0(x)=0$

- 对于$m=1,2,...,M$

  - 计算当前轮次的残差$r_{mi}=y_i-f_{m-1}(x_i)$
  - 依据残差，学习一颗回归树$T(x;\Theta)$
  - 更新模型$f_m(x)=f_{m-1}(x)+T(x;\Theta)$

- 得到最终的回归问题提升树：
  $$
  f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
  $$



## 一般问题：梯度提升树

当损失函数是一般的函数形式时，直接最小化经验风险往往是很困难的，因此可以采用最速下降法近似求解。主要思想是利用损失函数的负梯度作为回归问题的残差近似值，拟合回归树。

梯度提升算法的流程如下：

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$,损失函数$L(y,f(x))$

输出：回归树$\tilde{f(x)}$

步骤：

- 初始化$f_0(x)$
  $$
  f_0(x)=\mathop{\arg\min}_{c}\sum_{i=1}^NL(y_i,c)
  $$

- 对于$m=1,2,...,M$

  - 对于$i=1,2,...,N$，计算
    $$
    r_{mi}=-\left[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\right]_{f(x)=f_{m-1}(x)}
    $$

  - 依据$r_mi$拟合一颗回归树，得到第m棵树的叶节点划分区域$R_{mj},j=1,2,...,J$

  - 对于$j=1,2,...,J$，计算
    $$
    c_{m,j}=\mathop{\arg\min}_{c}\sum_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i)+c)
    $$

  - 更新$f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x \in R_{mj})$

- 得到回归树：
  $$
  f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x \in R_{mj})
  $$
  

![img](../img/format,png)

从不同损失函数的负梯度看，其他提升树方法可作为梯度提升树模型的特例。