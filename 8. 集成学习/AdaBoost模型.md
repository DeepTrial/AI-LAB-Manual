# AdaBoost模型

## 1.算法流程

Adaboost算法属于Boosting方法，即串行生成T个基学习器，每个基学习器间存在依赖关系。

文字版简要流程如下：

- 初始化训练数据的权值
- 串行训练个体学习器

  - 训练个体学习器

  - 对于训练数据，如果当前个体分类器将其正确分类，则其权值降低，反之升高
  - 对于个体学习器，如果它的分类误差较小，则该学习器的权值较大，反之较小
  - 依据新的训练数据权值，训练下一个学习器，直到生成T个
- 将T个学习器组合，使用加权和的方式。学习器的权值于其分类误差相关。



以二分类模型为例，AdaBoost的具体流程如下：

### 1.1 初始化数据集权值

对于包含$n$条训练数据的数据集 $D=\{(x_1,t_1),(x_2,t_2),...,(x_n,t_n)\}$，在Adaboost算法开始时将每条数据的权值（分布）初始化为$\frac{1}{n}$ , 得到第一阶段的训练集分布$D_1$，即:
$$
D_1=\{w_{11},w_{12},...,w_{1n}\}\quad and \quad\forall w_{1i}=\frac{1}{n}
$$
  

### 1.2 迭代训练$T$个个体学习器

使用循环方式，串行生成T个个体学习器，对于第m次循环，数据集的权值为$D_m$,循环内的操作如下：

- （a）：使用权值分布为$D_m$的数据集训练一个个体学习器$G_m(x)$ 。训练以最小化分类误差$e_m$为目标:
  $$
  e_m=\sum_{i=1}^{n}w_{mi}I(G_m(x_i)\neq t_i)
  $$
  其中函数$I(\cdot)$是指示函数，当括号内表达式为True是函数值为1，反之为0。由此训练出的个体学习器将优先确保权值较大的数据被判断正确。

- （b）：计算个体学习器$G_m(x)$的组合权值$a_m$，该值表示当前学习器在集成模型中的重要程度。$a_m$的值仅与当前学习器的误差有关，且随着$e_m$的减小而增大。
  $$
  a_m=\frac{1}{2}log\frac{1-e_m}{e_m}
  $$
  

- （c）：更新训练集的权值，生成下一轮循环的权值$D_{m+1}$。更新依据 被误分类的数据权值增大，反之减小的原则进行。
  $$
  \begin{cases}
  w_{m+1,i}=\frac{w_{mi}}{Z_m}e^{-a_mt_iG_m{x_i}}, i=1,2,...,n\\
  Z_m=\sum_{i=1}^nw_{mi}e^{(-a_mt_iG_m(x_i))}
  \end{cases}
  $$
  其中$Z_m$是规范化因子，确保$D_{m+1}$是一个分布。上式还可以简化为：
  $$
  w_{m+1,i}=\begin{cases}
  \frac{w_mi}{Z_m}e^{-am}, \quad G_m(x_i)=t_i\\
  \frac{w_mi}{Z_m}e^{am}, \quad G_m(x_i)\neq t_i
  \end{cases}
  $$
  

### 1.3组合个体学习器

在循环完成后，将$T$个弱分类器组合：
$$
f(x)=\sum_{m=1}^Ta_mG_m(x)
$$
对于二分类问题，**$f(x)$的符号表示分类结果，$f(x)$的绝对值表示确信度**。 可以添加一个符号函数（$sign(\cdot)$当括号内表达式大于0时返回1，小于0时返回-1，等于0时返回0）用于输出最终的分类结果。
$$
G(x)=sign(f(x))=sign\left(\sum_{m=1}^Ta_mG_m(x)\right)
$$

## 2. 算法推导

统计学习方法P162-P165（西瓜书P173-P176）给出了Adaboost的算法推导过程，证明了算法的正确性，并给出上文各项公式的来源，请自行参考。

简而言之，AdaBoost算法与加法模型，损失函数为指数函数，学习算法为前向分布算法的二分类学习方法一致（只相差规范化因子$Z_m$）。AdaBoost是前向分布加法算法的特例。